{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Youtube Videos Transcription with OpenAI's Whisper**\n",
        "\n",
        "[![blog post shield](https://img.shields.io/static/v1?label=&message=Blog%20post&color=blue&style=for-the-badge&logo=openai&link=https://openai.com/blog/whisper)](https://openai.com/blog/whisper)\n",
        "[![notebook shield](https://img.shields.io/static/v1?label=&message=Notebook&color=blue&style=for-the-badge&logo=googlecolab&link=https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)](https://colab.research.google.com/github/ArthurFDLR/whisper-youtube/blob/main/whisper_youtube.ipynb)\n",
        "[![repository shield](https://img.shields.io/static/v1?label=&message=Repository&color=blue&style=for-the-badge&logo=github&link=https://github.com/openai/whisper)](https://github.com/openai/whisper)\n",
        "[![paper shield](https://img.shields.io/static/v1?label=&message=Paper&color=blue&style=for-the-badge&link=https://cdn.openai.com/papers/whisper.pdf)](https://cdn.openai.com/papers/whisper.pdf)\n",
        "[![model card shield](https://img.shields.io/static/v1?label=&message=Model%20card&color=blue&style=for-the-badge&link=https://github.com/openai/whisper/blob/main/model-card.md)](https://github.com/openai/whisper/blob/main/model-card.md)\n",
        "\n",
        "Whisper is a general-purpose speech recognition model. It is trained on a large dataset of diverse audio and is also a multi-task model that can perform multilingual speech recognition as well as speech translation and language identification.\n",
        "\n",
        "This Notebook will guide you through the transcription of a Youtube video using Whisper. You'll be able to explore most inference parameters or use the Notebook as-is to store the transcript and video audio in your Google Drive."
      ],
      "metadata": {
        "id": "96kvih9mXkNN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "IfG0E_WbRFI0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc63d975-e072-475c-f5ec-6b616627bd5f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.5/39.5 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m19.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m111.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using device: cuda:0\n"
          ]
        }
      ],
      "source": [
        "! pip install -Uq yt-dlp\n",
        "! pip install -Uq transformers\n",
        "! pip install -Uq faster-whisper\n",
        "\n",
        "import sys\n",
        "import warnings\n",
        "# import whisper\n",
        "from pathlib import Path\n",
        "import yt_dlp\n",
        "import subprocess\n",
        "import torch\n",
        "import shutil\n",
        "import numpy as np\n",
        "from IPython.display import display, Markdown, YouTubeVideo\n",
        "\n",
        "device = torch.device('cuda:0')\n",
        "print('Using device:', device, file=sys.stderr)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#@markdown ---\n",
        "#@markdown #### **Youtube video or playlist**\n",
        "URL = \"https://www.youtube.com/watch?v=UdxSCFmUk9o\" #@param {type:\"string\"}\n",
        "\n",
        "video_path_local_list = []\n",
        "\n",
        "ydl_opts = {\n",
        "    'format': 'm4a/bestaudio/best',\n",
        "    'outtmpl': '%(title)s.%(ext)s',\n",
        "    # ℹ️ See help(yt_dlp.postprocessor) for a list of available Postprocessors and their arguments\n",
        "    'postprocessors': [{  # Extract audio using ffmpeg\n",
        "        'key': 'FFmpegExtractAudio',\n",
        "        'preferredcodec': 'wav',\n",
        "    }]\n",
        "}\n",
        "\n",
        "with yt_dlp.YoutubeDL(ydl_opts) as ydl:\n",
        "    error_code = ydl.download([URL])\n",
        "    list_video_info = [ydl.extract_info(URL, download=False)]\n",
        "\n",
        "for video_info in list_video_info:\n",
        "    video_path_local_list.append(Path(f\"{video_info['id']}.wav\"))\n",
        "\n",
        "for video_path_local in video_path_local_list:\n",
        "    if video_path_local.suffix == \".mp4\":\n",
        "        video_path_local = video_path_local.with_suffix(\".wav\")\n",
        "        result  = subprocess.run([\"ffmpeg\", \"-i\", str(video_path_local.with_suffix(\".mp4\")), \"-vn\", \"-acodec\", \"pcm_s16le\", \"-ar\", \"16000\", \"-ac\", \"1\", str(video_path_local)])\n"
      ],
      "metadata": {
        "id": "xYLPZQX9S7tU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6043d9d-67b1-4e9b-beca-abaf7873d87e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=UdxSCFmUk9o\n",
            "[youtube] UdxSCFmUk9o: Downloading webpage\n",
            "[youtube] UdxSCFmUk9o: Downloading tv client config\n",
            "[youtube] UdxSCFmUk9o: Downloading player e7567ecf\n",
            "[youtube] UdxSCFmUk9o: Downloading tv player API JSON\n",
            "[youtube] UdxSCFmUk9o: Downloading ios player API JSON\n",
            "[youtube] UdxSCFmUk9o: Downloading m3u8 information\n",
            "[info] UdxSCFmUk9o: Downloading 1 format(s): 140\n",
            "[download] Destination: Stanford CS153： Infra at Scale - Anthropic Cofounder Ben Mann on Scaling Frontier AI Systems.m4a\n",
            "[download] 100% of   38.83MiB in 00:00:01 at 24.11MiB/s  \n",
            "[FixupM4a] Correcting container of \"Stanford CS153： Infra at Scale - Anthropic Cofounder Ben Mann on Scaling Frontier AI Systems.m4a\"\n",
            "[ExtractAudio] Destination: Stanford CS153： Infra at Scale - Anthropic Cofounder Ben Mann on Scaling Frontier AI Systems.wav\n",
            "Deleting original file Stanford CS153： Infra at Scale - Anthropic Cofounder Ben Mann on Scaling Frontier AI Systems.m4a (pass -k to keep)\n",
            "[youtube] Extracting URL: https://www.youtube.com/watch?v=UdxSCFmUk9o\n",
            "[youtube] UdxSCFmUk9o: Downloading webpage\n",
            "[youtube] UdxSCFmUk9o: Downloading tv client config\n",
            "[youtube] UdxSCFmUk9o: Downloading tv player API JSON\n",
            "[youtube] UdxSCFmUk9o: Downloading ios player API JSON\n",
            "[youtube] UdxSCFmUk9o: Downloading m3u8 information\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "-X0qB9JAzMLY",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "\n",
        "from faster_whisper import WhisperModel, BatchedInferencePipeline\n",
        "\n",
        "model = WhisperModel(\"deepdml/faster-whisper-large-v3-turbo-ct2\", device=\"cuda\", compute_type=\"float16\")\n",
        "batched_model = BatchedInferencePipeline(model=model)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "segments, info = batched_model.transcribe(str(video_path_local), batch_size=16,\n",
        "                                          language=\"en\",\n",
        "                                          )"
      ],
      "metadata": {
        "id": "Ad6n1m4deAHp"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85GIYltvdKok",
        "outputId": "3f342e90-e8a7-4e8c-ee39-18a968eff797"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Detected language 'en' with probability 1.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results = []\n",
        "for segment in segments:\n",
        "    print(\"[%.2fs -> %.2fs] %s\" % (segment.start, segment.end, segment.text))\n",
        "    results.append([segment.start, segment.end, segment.text])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HRf6mNQfdozY",
        "outputId": "486701ee-cecc-463f-ad66-7c1326c43955"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[1.17s -> 29.09s]  Well, thanks for coming, Ben. Thanks for having me. Give us a sense of the scale that Anthropic's at right now. Yeah, so in terms of actual numbers, I don't want to give specifics, but you can Google it and see what you find. What I can say is that in the last year, we've 10xed our revenue, and we've, in the last three months leading up to December, we 10xed our revenue just in the coding segment.\n",
            "[29.09s -> 54.61s]  So we're seeing absolutely explosive growth in all areas and having a pretty fun time trying to serve all that traffic. And how did you get to where you are now? In life? Yeah. Yeah. I guess I started thinking about computer science in undergrad and wasn't one of these people who starts coding when they were five.\n",
            "[54.67s -> 84.34s]  And I just fell in love with it. I originally thought I wanted to be a mechanical engineer and do robotics, but I hated mechanical engineering and I hated robotics when I took the intro classes. And computer science just kind of stole my imagination, ended up doing the AI track at Columbia. And back then, AI was pretty different. We were talking about things like artificial intelligence leading to systems that could think like people do, but it was still pretty far away and it wasn't.\n",
            "[84.34s -> 113.86s]  practical by any means. We were talking about things like expert systems in the AI winter of the 80s, multilayer perceptrons, which are the protozoan ancestors of the models that we have today. And that definitely caught my imagination. And so I worked at Google for a while intending to just sort of learn the ropes and immediately start a company, which I imagine is pretty familiar to an audience like Stanford. But I kept on learning more things.\n",
            "[113.86s -> 143.66s]  But when ImageNet came out in 2015, for me, that was a tectonic moment where suddenly these techniques that people had been talking about for a long time were suddenly practical in ways that they hadn't been before on tasks that typically would have required a human judge to decide, you know, what kind of dog is this? And it was definitely way better at classification than I was and could be trained on a single GPU, which was amazing.\n",
            "[143.66s -> 173.54s]  From there, I started jumping into AI and learning about it just on my own. Didn't do a master's, didn't do a PhD, just started reading a lot of papers and figuring it out. Worked with a bunch of people who were starting companies to get my feet wet. And shortly after that, I decided to join OpenAI. I had read this book called Super Intelligence by Nick Fostrom that talked about how AI is going to be this really important thing for humanity as a whole to figure out, not just America or\n",
            "[173.54s -> 202.96s]  your school, your state or whatever. It'll be this thing that we need to existentially grapple with. And it seemed right to me. So I joined OpenAI in 2017. And at that time, we had no idea how we were going to get to AGI. We just thought stuff is happening in AI and we should get on the frontier. And I really bought the safety mission at the time. And I think there's some questions about how\n",
            "[202.96s -> 232.35s]  adherent they are still to that mission today, but definitely making some huge progress. And then I'll fast forward a bit. When GPT-2 came out, I was like, aha, this is how we get to AGI. It won't be that we have some simulated agents on a desert island who are kind of battling it out and trying to have intelligence emerge. Instead, we'll train it on all the world's knowledge in the form of data from the internet. And from there,\n",
            "[232.35s -> 259.10s]  it will exhibit properties of human intelligence. And a lot of people poo-pooed that and they were like, oh, it's just pattern matching. It's not real. It's not reasoning. Humans are somehow special. But even at that time, I believed that it was just a continuous ramp to getting to the capabilities that we have today and tomorrow. And so I joined OpenAI again after an interlude. And at that time, I ended up...\n",
            "[259.10s -> 288.99s]  with Dario and Tom Brown starting the GPT-3 project. So I was one of the first authors on that paper. Even though I didn't have a formal research background, I did all the data engineering and a lot of analysis for how data affected the quality of the model that came out of that process. Did a bunch of architecture experiments with Tom. And to me, that was sort of the confirmation that scaling laws could hold up across 13 orders of magnitude.\n",
            "[288.99s -> 318.56s]  back then, much more now. And it's actually just very rare, even in the physical world, that we see any phenomena that persist across that kind of scale. So that was quite inspiring to me. And then at some point, we felt like we could make safety more a core part of our mission if we left to start our own company. So four years ago, eight of us left the company to start Anthropic. And since then, we've kind of\n",
            "[318.56s -> 346.40s]  leapt to the frontier. As the people who were building GP3 at OpenAI, we had a much easier time doing it the second time around. And on top of that, we were able to make a bunch of safety breakthroughs that ended up being really commercially valuable as well, which wasn't something that was for granted, wasn't necessarily going to happen, but ended up being really important for us to create a race to the top, which is what we call\n",
            "[346.40s -> 372.05s]  sort of pushing all the other companies that are in the field to at least match us in their commitments to making safe AI. Let's focus on the GPT-3 era for a bit. One of the things that has been a very consistent theme of computing systems across\n",
            "[372.05s -> 402.03s]  infrastructure primitives like storage, networking, compute, is that other than Moore's law, which has been fairly predictable and has not really diminished for the greater part of the 30 years or so that we've had the chance to empirically verify it and somewhat predictive, most other computing performance metrics start off accelerating pretty exponentially and then hit sigmoids. They start plateauing. If you look at latency between\n",
            "[402.03s -> 429.73s]  interconnects, chips, bandwidth latency over the internet. If you look at the CPU performance too, right? Like we did have this extraordinary moment of exponential improvement on most of these computing system improved performance metrics in the 90s, right, and early 2000s. And then they kind of all start to plateau. And so for a lot of infra veterans from the cloud era, when GPT-3 was first published, it was, yeah, yeah, yeah, we've seen this before.\n",
            "[429.73s -> 459.07s]  but this isn't going to hold. And there was a lot of skepticism. What was it internally that made you go, even though you didn't have a traditional, you didn't have a physics background, you didn't have a PhD, and you were actually more likely from your prior biases to have the engineering biases saying, well, these systems will plateau out. What prevented you from falling into that trap? Yeah, I might actually dispute the premise that things like memory bandwidth and other systems besides CPUs.\n",
            "[459.07s -> 485.33s]  more sigmoided. I think there are other factors that pushed that to happen. And now that, for example, fast interconnects are something that people really care about, now they're like continuing more of an exponential ramp. That it was just that those systems were not the bottleneck for the systems to continue to improve overall. And that you'd say it was the lack of investment in those fundamental research breakthroughs, not...\n",
            "[485.33s -> 515.09s]  A fundamental limitation of the computing system. So like the compute service providers, they said, you know, we have 30 gigabit interconnects between the machines and our data centers, and nobody is asking us for more. So we're not going to do anything. And then NVIDIA acquires Mellanox, which has like 400 gigabit interconnects. And now everybody's like, oh shit, now we have to, we have a lot more interconnects. And now the pace of innovation for interconnects is increasing a lot.\n",
            "[515.09s -> 542.58s]  again. And if you look at Apple's M-series of chips, their memory bandwidth is really remarkable. That's one of the main things that differentiates their M-series from the traditional AMD or Intel Silicon. So is your sense that the skepticism around scaling laws then came mostly from observing plateaus in performance evals for previous AI?\n",
            "[542.58s -> 572.18s]  models? Is that where the skepticism? Why do you think there was so much inner for resistance to the idea of scaling laws? I think there was resistance because people felt that it wouldn't just keep happening. And as an example of this, right before the GPT-3 paper came out, a paper called T5 from Google was published. And it was about an 11 billion parameter model. And in the concluding paragraph of that paper, they said, we don't see any returns to scale.\n",
            "[572.18s -> 601.07s]  We think that even T5 is undeployable as it is. Like nobody will spend the amount of money that it costs to do inference on this thing. People were locked in the sort of BERT paradigm where they were doing like 300 million parameter models and that was considered large. And so to scale out to 11, it would take like an entire GPU box with many accelerators doing data parallel operations.\n",
            "[601.07s -> 623.68s]  people didn't think it was going to work economically, let alone from a performance standpoint. So I think this has happened in a bunch of different areas. So if you look at even like the fastest mile that a person can run, there's this thing called the banister effect. Before somebody broke the four minute mile, people thought that it was impossible.\n",
            "[623.68s -> 647.04s]  And then as soon as somebody did, then everybody started doing it. And so I think it's pretty common for people to have a more conservative worldview of like, I've never seen this happen. And it seems unlikely because I don't have any priors of it happening. So I'm just going to assume it can't happen. That's one piece. And the second piece is assigning something special to human cognitive ability.\n",
            "[647.04s -> 676.66s]  And if you look at people like Francois Chalet who talk about reasoning in the ARC Prize, I think there's a big question of like, are the reasoning models something special or is that just a capability that you could have elicited that is just weaker if you don't train it specifically? And so elicitation is also a big part of this question of how good are the models and how much performance can we extract on a certain benchmark?\n",
            "[676.66s -> 704.88s]  Once it started to become clear that if you did scale, you would start to see, as long as you threw enough compute and invested in a sufficient flops budget, that you could improve elicitation, especially on adversarial metrics like safety. What were the systems that then, once we had crossed that barrier of saying, okay, it's holding, we've got enough compute to throw at this.\n",
            "[704.94s -> 727.22s]  What were the systems that then started keeping you up at night that you were concerned would break? I mean, I think it was kind of everything at every level of the stack. And I also feel like a big part of why things didn't go faster is that it required a really tight collaboration between researchers and engineers. Where typically, if you look at...\n",
            "[727.22s -> 749.68s]  DeepMind in the early days, it was mostly researchers who were driving the ship, and the engineers were like the lackeys who they would throw tasks over to. No offense to any of them. But at OpenAI, it was very integrated and anthropic, I would say, even more so. Right. Where they're like cohesive teams that are steering the ship together.\n",
            "[749.68s -> 779.31s]  These things start becoming, start looking more like engineering mega projects, you know, like Three Gorges Dam style. Like we have to get a ton of resources into this one thing. And I think that's another reason that DeepMind wasn't set up for success or Google Brain, where typically they organized as a bunch of researchers each doing their own thing. And if they wanted to pool their resources to do one big thing, then they couldn't really convince anybody.\n",
            "[779.31s -> 807.90s]  make a big bet. Whereas at OpenAI and later at Anthropic, I think from the beginning, we've been doing these big bets and they are quite risky, but the magic of scaling laws is that it allows you to turn what was previously an art where you kind of just throw stuff at the wall and see what sticks into more of a science to say, look, we know what the scaling looks like in terms of...\n",
            "[807.90s -> 835.95s]  hyperparameter selection, dataset quality, and we can do these small, cheap experiments that will then give us confidence that by the time we scale up, it will be a useful artifact instead of a very expensive piece of trash. That's not an analogy I've heard before, but that's an interesting way to describe it. If you had to contrast that moment when you were training GPT-3 with\n",
            "[835.95s -> 865.78s]  a system like Claude 3.5 Sonnet, which is roughly state-of-the-art now, what's the biggest difference in the hardest engineering problems you had to tackle then versus now? I mean, one of the problems is compartmentalization, where now we have hundreds of people working on these models, and we need all of them to be able to coordinate with each other, but we don't want our compute multipliers, which is our secret sauce of, you know, given techniques that\n",
            "[865.78s -> 893.06s]  raise the capabilities for a certain compute budget, we don't want those to leak. So we've borrowed these techniques that the American intelligence organization uses, for example, where only some people know about some of the techniques. Or I guess silicon CPU developers also have used these for a long time. So no one person can hold the whole system in their heads, and yet you still need to create\n",
            "[893.06s -> 921.49s]  an artifact, which is a cohesive whole at the end. So we have methods that we use to try to make that successful. That's definitely one challenge. From a systems engineering perspective, we also don't control our own compute. We rely on Amazon, Google, our other compute providers to orchestrate our clusters on our behalf. But our workloads look completely different from many other workloads. We're using Kubernetes clusters with very high numbers of nodes.\n",
            "[921.49s -> 949.92s]  out of spec for what the standard is supposed to support and we're pushing these systems to their limits on pretty much every axis whether it be the reliability of being able to if a job fails like if one if one machine in a vastly distributed job fails being able to restart quickly and not lose a lot of progress the cloud storage of storing all the snapshots and transmitting the data to the to the machines for the training\n",
            "[949.92s -> 976.85s]  and now the big thing is reinforcement learning which is even more complicated where you'll have some kind of stateful environments that the agents are interacting with and those agents need to have the most recent model weights and efficiently updating them so yeah it's it's hard at every level and new stuff breaks every day and uh you know even at openai we had we had a bug where we flipped a negative sign on the reward and the model was\n",
            "[976.85s -> 1005.02s]  seeming to get more and more evil as we trained it. It was a preference model reward, I think. And at some point, we realized that actually there is a double negative. And so the bug had been there for a long time. And when we fixed it, we actually broke it. So then we had to fix it twice. Yeah, so let's talk about that incident for a sec.\n",
            "[1005.02s -> 1032.72s]  Was that for 3 or 3.5? I wasn't there for 3.5. Okay, so it's for 3. Okay. So at this point, 3 had already been done training. It was in prod. This was a bug that was live. No, no. I think this was in some underlying test model. We never release a model without red teaming it. And we have all these methods of rapidly evaling it over time while it's in training to make sure that things look healthy.\n",
            "[1032.72s -> 1059.33s]  And I guess to give a sense of these runs, you never just like YOLO put it out there and then hope that it works. While it's running, you're constantly watching like hundreds of different diagnostics to make sure that it's healthy and thriving. And if you've trained an ML model, you know that there are these loss curves that you watch on the training distribution or other distributions. And for big models in particular, you often get loss spikes.\n",
            "[1059.33s -> 1087.62s]  So sometimes you just like roll it back and hope that when you roll it forward again, it doesn't spike, even though you haven't changed anything. And if the spikes get too hard, then you have to do a more deep surgical intervention. So it's very much an art. And it also feels kind of like you have a patient on life support or like it feels very critical to like always be watching it. Right. And...\n",
            "[1087.62s -> 1116.70s]  Yeah, so I guess coming back to the question of, like, was it in Prague? No. This is just during training. We noticed stuff was going wrong. Yeah. And maybe this is in the post-training. But, yeah. Yeah, this is taking me back to 2021. I think it was maybe June or July when I was out with Tom, who was one of your co-founders. We were at a birthday or something, and Tom kept nervously refreshing. I think it was like...\n",
            "[1116.70s -> 1143.73s]  some observability dashboard, and it was basically babysitting the run, which was a common thing back in the day. Dashboarding, alerting, observability, monitoring was quite brittle, and so you literally had somebody babysitting the run for these situations. How has that changed now? Are things any better at all, or is it still being for teams in a war room for every training run, every ablation? Observability is just as brittle as it was then.\n",
            "[1143.73s -> 1167.17s]  I mean we definitely have borrowed a lot of tricks from standard engineering practice in terms of having on-call rotations and trying to make things sustainable and even having like follow the sun rotations where we have people all around the world for whom it's daytime so you don't have to get called in the middle of the night and go clean up the models pooping the bed or whatever.\n",
            "[1167.17s -> 1195.15s]  Yeah. Ben has two kids. I have two babies. Which body of analogies to draw on here? Six-month build and a two-year-old. But yeah, I think it's still pretty hard. I'm not going to sugarcoat it. So take us into the training of one of these systems today that are, from a complexity point, have ballooned dramatically because of the sheer scale relative to GP3 now.\n",
            "[1195.15s -> 1224.37s]  roughly 10 orders of magnitude higher at least on size. Claude is, you know, it's not public how large Claude is and so on, but at least by that much, right? So you've scaled the size of the models. You've scaled the team by 10x that's working on it. And then roughly scaled the number of customers relying on the model in production is on inference by well north of 10x. Is that right? So that's one order of magnitude.\n",
            "[1224.46s -> 1254.29s]  But actually, I think your users relative to, I know you weren't there for 3.5, but roughly orders of magnitude wise, how many users do you think Anthropic is, how many orders of magnitude more users is Anthropic supporting inference today than year one? So many more. I mean, year one, we started from zero. Fair enough. Let's call the first Claude. Actually, let's use Claude version that had been done training in,\n",
            "[1254.29s -> 1282.21s]  March 2022. Yeah. Pre-ChatGPT. Yeah. Because that had a few thousand users, right? And friends and family. Yeah. So before ChatGPT came out, we had a friends and family version of Claude that people could use in Slack. And people really liked it. There is a big question of what would it mean for us to expose that to the world? And we had a lot of debates about it internally of what that would do to the zeitgeist.\n",
            "[1282.21s -> 1311.92s]  And our general feeling was that it would cause too much acceleration. Ironically, there's a rumor that ChatGPT launched because they thought we were about to launch something, which wasn't true. But I still feel pretty good that we gave the world six more months to work on safety. In terms of that model, it was pretty basic. And we hadn't done a ton of optimizations to make it fast to run or make it...\n",
            "[1312.08s -> 1338.90s]  like, comfortable to use. But just using it in Slack, I think, woke up a lot of people who were in that friends and family program. I guess, what was your impression as one of those testers? I thought we should let her rip. Yeah. I mean, it's extraordinary. You know, I had the privilege of being one of Anthropic's first investors. You and Dario and Tom gave me a call at the end of 2020 and said, we want to leave OpenAI and start Anthropic.\n",
            "[1338.90s -> 1368.90s]  I said, sure, what do you think we're going to need to raise to get to our first milestone? And without blinking, Darius said 500 million. And I was like, well, that's going to take a while. And how long do you think it's going to take us to get to our first training run that would be comparable? And there was some debate about whether we could initialize the training run with open source weights. I think Megatron 530B was like a candidate at the time. But I mean, the closest estimate at the time of getting to something of that level was going to be like seven to nine months at least. Yeah.\n",
            "[1368.90s -> 1396.37s]  But you guys trained it way faster, and it was extraordinarily coherent. I think that was the first observation I had of using the bot in Slack, was that GPT 2.5 was like your kind of chaotic friend on drugs, you know, a little bit. Like, it was fun, but you couldn't have really a sustained, coherent conversation. With Claude, it was clear that it had crossed the barrier.\n",
            "[1396.37s -> 1421.39s]  of maintaining character of being a helpful, harmless assistant. Was that actually a system prompt? Was that explicitly designed? How did you guys get it to do that? Yeah, I think there are a couple of things. One is that as models improve in quality, they just become more coherent naturally. So if you've ever used GPT-2, which maybe none of you have, but I guess small models also exhibit this behavior.\n",
            "[1421.39s -> 1449.79s]  Even from like sentence to sentence, it can kind of lose the thread or if you write it to low temperature, it will just start repeating itself over and over again. Like if you ever try to write prose in GitHub Copilot, it will do this very frequently. And so part of it is that we just had a better model. And then I think the other part is the state of the art at the time was instruction tuning. And that's typically like a single turn interaction.\n",
            "[1449.79s -> 1477.76s]  So once you get beyond a single turn, you start having these more dialogue-like interactions. Yes, right. The model is what we call outer distribution, where it hasn't seen us in training and it kind of gets confused and performance falls off a cliff. So for our training runs, we very early on started collecting human feedback. Right. And that human feedback was always multi-turn from the start. And the prompt that we gave it...\n",
            "[1477.76s -> 1500.64s]  which I guess was sort of the early kind of system prompt, was another dialogue. So it was already trying to model this behavior. And as our models improved and we would rapidly roll in the feedback that the humans gave us back into the training process and then put those back in the interface and sort of Ouroboros our way up the chain. Right.\n",
            "[1500.64s -> 1529.81s]  it would get better and better at maintaining coherence over long conversations. Yeah, so that was RLHF, which you guys pioneered at OpenAI. Just a quick detour here. How many folks know what RLHF is? Okay, never mind. We're not going to explain it then. Most people know it. My last question for you is, that was the first iteration of RLHF. Why, from a systems perspective, did you guys go from betting on that to RLHF? And could you explain for the folks?\n",
            "[1529.81s -> 1558.48s]  what the meaningful difference is. Yeah, so RLHF, you're asking humans to submit preferences, and then those preferences train a preference model that then stands in for the human during reinforcement learning and says whether a given output is good or bad. So you can think of it as we're training a teacher, and then the teacher is training the student, which is the model that you end up seeing at the end.\n",
            "[1558.54s -> 1586.99s]  So, and then you kind of throw the teacher away when you're done in a nice way. For RLAIF, it's very different. That's what we call constitutional AI, right? That's one way of doing RLAIF, where we write a set of natural language principles that defines how we want the agent to behave, which may be something like be kind or be empathetic or don't write cybersecurity attacks or like don't create...\n",
            "[1587.25s -> 1614.13s]  recipes for poison or something. And then in a completely enclosed process with no humans in the loop, we ask the model to critique itself. And then based on its own critiques, update itself. So this process is much more steerable because humans, they come from very different backgrounds. Maybe they will interpret the instructions that we give them differently. Maybe they...\n",
            "[1614.13s -> 1644.02s]  won't remember all the instructions that we gave them. With RLA-IF, it's a very repeatable, amenable to science process that we can iterate on in a laboratory setting and really tune it to get exactly the outputs that we want. But it's only possible beyond a certain capability threshold. And if you try to do it with too small a model, it just won't work. Because when you ask the model, did you comply with principle X when you said this sample output?\n",
            "[1644.02s -> 1674.02s]  It won't give you a good answer and it won't be able to revise in response to that. But with strong enough models, then you can bootstrap to higher capability levels and then keep on recursively self-improving. The reason I think this is important is because for many of you who are software engineers wondering how do you contribute to the frontier, hearing Ben talk about the RLEI pipeline, it should become clear that it's an engineering challenge. It's not just a research challenge.\n",
            "[1674.02s -> 1703.42s]  I know a big part of Anthropic has been making sure that it's a great place for engineers to be able to contribute because the fundamental computing and infra challenges are not just research to get the more capabilities. Yeah. Feel free to repeat it. Yeah, so he's referring to an interview that I did with Kevin Roos maybe a year or two ago on Anthropic's take on AI safety. I think the title is something like...\n",
            "[1703.42s -> 1733.02s]  the AI doomers in San Francisco, something, something. And the question was, in the interview, I said something like, we're aware of the things that can and do go wrong with AI system training. How do you do that evaluation? And how do you anticipate the new things that might come up? That's a good question. Evals are extremely hard. We actually have a blog post on why evals are hard for the gory details. But the short answer is,\n",
            "[1733.02s -> 1762.26s]  We're constantly trying to make our evaluation better. And we've made our policy of evaluation public and how we do those evaluations in our responsible scaling policy. So you can go look at the document, but basically it says we care most about things like CBRN risks, which are chemical, nuclear, radiological, and biological risks. These are things that could really destabilize society if models were capable of doing these things.\n",
            "[1762.26s -> 1791.26s]  But we also look at things like cybersecurity capabilities and things like that. And we work with expert red teamers who, you know, you can go hire pen testers from a cybersecurity company. We also work with people who you can't really hire, who work for the U.S. government and known national nuclear secrets and work with them to elicit these capabilities from the models. That said, there is an elicitation overhang, which means...\n",
            "[1791.26s -> 1820.22s]  even if a model might have a capability latent in it, there may be a special way of asking the model for that capability that we don't know yet. So as an example of this, historically, Chain of Thought came about pretty recently, where if you ask the model, show me your thought process step by step, then it dramatically improves the model's outputs. Even saying something as simple as, try very, very, very, very hard, and all those varies are actually important.\n",
            "[1820.22s -> 1841.31s]  That also improves model output, or it did back when this research was done. So it's definitely an ever advancing science. We collaborate with the United States and UK AI safety institutes. Basically, it's a super hard problem. We need more people thinking about it. And I actually think academic settings is...\n",
            "[1841.31s -> 1869.50s]  one of the best ways to contribute to things like evals, where you don't actually need a ton of resources to get a sense of what the model's capabilities are. If you look at benchmarks like OS World that came out recently, these give us a sense of reproducibly across many model providers, how good is the model at manipulating a desktop computer with a keyboard and mouse. And while that in itself is not risky, it can point to\n",
            "[1869.50s -> 1897.92s]  risks like usage for cyber attacks. So we definitely care a lot about this and we spend a ton of time thinking about it. We collaborate with a bunch of people who spend a ton of time thinking about it. But we need more brains, so please help us. And maybe more, you know, sort of directly asking your question, as far as I'm aware, Anthropik is the only lab\n",
            "[1897.92s -> 1927.36s]  that is committed to pausing development based on capability thresholds using the RSP. There's still no other frontier lab that is committed to doing that. Is that right? Yeah. So specifically what we've committed to is each of our AI safety levels or ASLs require a certain set of mitigations that we've written down and pre-committed to. Right. So for example, ASL3 is the next one. And it's basically like if the model has some ability to create\n",
            "[1927.36s -> 1945.06s]  biological threats by itself can marginally accelerate a human researcher who's trying to do this stuff, but isn't necessarily superhuman at doing these things. If a model meets those criteria, then we need to have certain...\n",
            "[1945.06s -> 1974.78s]  controls in place like two-party control for committing code. So you can't just single-handedly modify the production environment for Anthropic. You need to have at least one other person review your actions. And that helps us with things like insider threat risk. And given that, that makes us more comfortable feeling like we won't be manipulated by our own financial or other market incentives to doing things that we don't think are safe.\n",
            "[1974.78s -> 2000.85s]  I think you're right that other companies haven't committed to this, or at least maybe not the frontier ones, but I'm not an expert on it. And then another thing that Antropic is unique at is having the LTBT, which is a governance mechanism that can shut down the company if the overseeing board decides that they're not developing AI in a way that's net positive for humanity. Anyway, that's something we could send out for reading, as far as I'm aware.\n",
            "[2000.85s -> 2030.13s]  Anthropic remains the only lab that's had a public LTVT governance set up. The question was, now that you can run frontier models locally, how does that affect how Anthropic does things in the future? I think the frontier will always only be able to run on data center level compute. And the things that you can run locally will lag behind them by a couple of years probably. And I think we were already seeing this with LLAMA models that\n",
            "[2030.13s -> 2055.94s]  You can already run Llama 300B on your machine. People like EXA had demos of networking two Macs together so you could run the biggest model. But of course, people are always improving quantization and shrinking that down. So from the standpoint of what capability you'll be able to get locally, yeah, that's always going to be increasing.\n",
            "[2055.94s -> 2084.26s]  But I think the frontier is the thing that we care about the most, especially from a safety standpoint. The reason it's important for us to stay on the frontier is that we want our safety techniques to be applicable to the frontier of AI models and the ones that are coming five to 10 years from now. So as long as we're able to stay ahead from that perspective, we'll be able to do our best safety work and sort of show everybody hopefully how to make big models safe, which...\n",
            "[2084.26s -> 2114.18s]  a lot of people didn't even think was possible before. One corollary of that question, I guess, is, you know, a big part of the large pre-training era has been having access to bigger and bigger single-spine data centers, right, where you have co-located chips on a single spine so you can interconnect larger and larger numbers of GPUs for a single training run. Do you think that continues to be the case for inference scaling? I mean, inference definitely doesn't need as much interconnects.\n",
            "[2114.18s -> 2142.70s]  Right. So you can distribute your compute more broadly. But for an inference scaling, for an RL training run over a long horizon chain of thought, is it more advantageous to have workloads running on a single spine? I don't think we know yet. I think pre-training is very efficient and can get you very far. And I don't think pre-training will disappear. Right. Like, I think there is some experiments with having a RL only run.\n",
            "[2142.70s -> 2168.58s]  But I think they still use distillation, which is a form of supervised learning. So it's not that different from a workload standpoint, from a technical perspective. But what will happen with RL, I think it's yet to be seen. There are lots of companies and open source organizations who have been trying to scale pre-training to...\n",
            "[2168.58s -> 2193.55s]  massively distributed networks of machines by doing single bit updates instead of full gradients. So that trend is already happening. Whether those are competitive with spending the money to get a giant data center, nobody knows yet. Certainly there's economic incentive for that.\n",
            "[2194.26s -> 2221.92s]  The question is how do we define safe AI and what are the mechanisms to make that happen? Safe AI is first and foremost AI that doesn't cause catastrophic harm to humanity and maybe on a more micro level does what you want and not what you say per se. Like you wouldn't want sort of a monkey paw style genie grants you three wishes thing as these agents become more powerful.\n",
            "[2221.92s -> 2249.57s]  So in terms of models today, we define it as ASL2, where even if you had unfettered access to the model and it wasn't that aligned, it wouldn't cause a lot of harm in society. So for example, today, even before our computer use stuff came out, models were pretty good at beating CAPTCHAs. There's not a huge amount of harm. There were already other techniques and more narrow models that could defeat CAPTCHAs. So the marginal risk.\n",
            "[2249.57s -> 2272.90s]  of having an LLM that could do that wasn't very high. However, as we get towards things that can do automated cyber attacks and things like that, the risk increases. And that's why we've defined these AI safety levels. In terms of mechanisms to prevent that, we have a defense in depth mindset, which comes from security, where\n",
            "[2272.90s -> 2296.34s]  We don't just have one system that tries to make the model safe. It's at every layer. So we incorporate safety training in pre-training and post-training. We have online classifiers that try to detect whether people are trying to do bad stuff that we call prompt shield. And we're developing more techniques every day to do scalable oversight.\n",
            "[2296.50s -> 2321.50s]  One of the things I'm most excited about is interpretability. Mechanistic interpretability is the flavor that we are most pushing forward. And the idea there is that if you could sort of peer into the mind of the model as it's operating, not just in the tokens that it's outputting, but how those concepts are even formed internally to the model, then it should be possible to audit.\n",
            "[2321.50s -> 2347.86s]  what a model is doing and make sure that it's not trying to do things like resource stockpiling or shutdown resistance that you would want to preserve in the case of an extremely powerful model. So we're seeing pretty good advancements in mechanistic interpretability. It's early days still, but I think we're kind of like Chris Ola and his team are...\n",
            "[2347.86s -> 2371.01s]  creating this field from whole cloth, which is pretty hard. And we're starting to get a sense of what's going on. So I think that's probably my greatest hope. I think we've got time for one more question. The question was, how do you think about authoring a foundation model at scale? And what's the difference between the API and the chat offerings?\n",
            "[2371.01s -> 2397.34s]  In terms of authoring, I mean, there's just so many different aspects to it. It's very much an art. We had this podcast with Amanda Askell and Stuart, can't remember his last name, who talked about our Claude character efforts, which are down in the details of how we think about refusals. And if you are refusing, can you still give a little bit of information to express empathy and to give the person...\n",
            "[2397.34s -> 2426.46s]  something to chew on, even if you didn't give them the answer that they wanted. So, yeah, that's a very broad question that's hard to answer. But maybe I'll focus on the second one, which is the difference between the chat experience and the API offering. The chat experience is actually much easier to move fast on and develop with because we control every aspect of it. And if we need to change something or pull it back,\n",
            "[2426.46s -> 2451.17s]  We can do that unilaterally. With an API, one of my coworkers who used to be at Stripe said APIs are forever. That once you release an API, now there are companies, partners who are depending on this API. And if you change it, they will be upset. You may break them. And so even deprecating Claude 1, Claude 2 models took us a very long time. We only just did it. And I think...\n",
            "[2451.17s -> 2480.03s]  Claude 2 is actually still in production somewhere, which is astounding given how much worse it is than all of our other models. But businesses have other considerations than just using the coolest stuff and the best stuff. For them, it might be more about business continuity or maybe they don't have engineering resources. So that's a huge difference between those two. And we also think of the chat experience as a way we can\n",
            "[2480.03s -> 2509.65s]  grant developers more power once we realize how good or not good a certain experience is. So, for example, you can now upload PDFs in the API. We had that in the chat experience for a long time, and we just wanted to make it easier for developers to be able to do that with Cloud because putting in context is one of the most important things. And I think you'll see that continuing over time that we use the chat experience as sort of a proving ground for the things that we've been exposed to other developers.\n",
            "[2510.58s -> 2514.77s]  Awesome. Thank you so much, Ben. Glad to be here. Thanks.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# save the text to f\"{video_info[\"title\"]}.txt\n",
        "with open(f\"{video_info['title']}.txt\", \"w\") as f:\n",
        "    f.writelines([r[-1] for r in results])"
      ],
      "metadata": {
        "id": "Zj8m_7MDe8OZ"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eofG2b9ggWKT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}